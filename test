#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import re
import json
import tldextract

import regex
from scipy.sparse import csr_matrix

from nltk.stem import snowball
from sklearn import svm

FNAME = "posts.json"

s = snowball.SpanishStemmer()

commonWords = [
    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'se', 'del', 'las', 'un',
    'por', 'con', 'no', 'una', 'su', 'para', 'es', 'al', 'lo', 'como', 'mas',
    'o', 'pero', 'sus', 'le', 'ha', 'me', 'si', 'sin', 'sobre', 'este', 'ya',
    'entre', 'cuando', 'todo', 'esta', 'ser', 'son', 'dos', 'tambien', 'fue',
    'habia', 'era', 'muy', 'anos', 'hasta', 'desde', 'esta', 'mi', 'porque',
    'que', 'solo', 'han', 'yo', 'hay', 'vez', 'puede', 'todos', 'asi', 'nos',
    'ni', 'parte', 'tiene', 'el', 'uno', 'donde', 'bien', 'tiempo', 'mismo',
    'ese', 'ahora', 'cada', 'e', 'vida', 'otro', 'despues', 'te', 'otros',
    'aunque', 'esa', 'eso', 'hace', 'otra', 'gobierno', 'tan', 'durante',
    'siempre', 'dia', 'tanto', 'ella', 'tres', 'si', 'dijo', 'sido', 'gran',
    'pais', 'segun', 'de', 'la', 'i', 'el', 'a', 'l', 'en', 'va', 'de', 'que',
    'del', 'un', 'les', 'amb', 'per', 'una', 'els', 'es', 'al', 'es', 'dels',
    'ser', 'van', 'o', 'com', 'mes', 'no', 's', 'seva', 'fou', 'entre', 'pero',
    'seu', 'com', 'per', 'tambe', 'son', 'als', 'on', 'aquest', 'anys',
    'ciutat', 'era', 'pel', 'despres', 'va', 'aquesta', 'part', 'gran', 'ha',
    'durant', 'sobre', 'any', 'nom', 'estat', 'altres', 'havia', 'dos', 'rei',
    'molt', 'primer', 'quan', 'fer', 'li', 'contra', 'pels', 'cap', 'els seus',
    'seves', 'pot', 'temps', 'mateix', 'hi', 'tres', 'forma', 'esta',
    'guerra', 'te', 'despres', 'fins', 'fill',

    'http', 'facebook', 'google', 'com', 'org', 'net', 'cat', 'es', 'www'
];

ACCENTS = ((u'à', u'a'), (u'á', u'a'), (u'è', u'e'), (u'é', u'e'), (u'ì', u'i'),
           (u'í', u'i'), (u'ò', u'o'), (u'ó', u'o'),
           (u'ù', u'u'), (u'ú', u'u'), (u'ñ', 'n'))

def clean(s):
    """ turns string to lowercase and removes all accents """
    s = s.lower()
    for c_old, c_new in ACCENTS:
        s = s.replace(c_old,c_new)
    return s

def cleanWord(word):
    if word in commonWords:
        return None
    return clean(s.stem(word))

def tokenize(message):
    urls = re.findall(regex.findUrl, message)
    found_urls = []
    for url in urls:
        found_urls += url.group(0)

    for url in found_urls:
        domain = tldextract.extract(url).domain
        message.replace(url, u"__url" + domain)

    tokens = re.findall(r"[\w']+", message)
    for t in tokens:
        t = cleanWord(t)
        if t is None:
            continue
        yield t

def run():
    with open(FNAME) as f:
        posts = json.load(f)

    for post in posts:
        data = post['text'][1:].decode('hex')
        data = data.decode('utf-8')
        print list(tokenize(data))


def generate_map(posts):
    feat_index = 0
    features = {}

    data = []
    row_ind = []
    col_ind = []

    postid = 0
    labels = []
    for post in posts:
        labels.append(int(post['state']))
        text = post['text'][1:].decode('hex')
        text = text.decode('utf-8')

        postfeatures = {}
        for token in tokenize(text):
            if token not in features:
                features[token] = feat_index
                feat_index += 1
            featureid = features[token]
            if featureid not in postfeatures:
                postfeatures[featureid] = 1
            else:
                postfeatures[featureid] += 1

        for (id, count) in postfeatures.items():
            data.append(count)
            row_ind.append(postid)
            col_ind.append(id)

        postid += 1
        if postid == 16000:
            break

    return (csr_matrix((data, (row_ind, col_ind)), shape=(postid, feat_index)), labels, features, feat_index)

def get_feature_vector(text, features, num_features):
    vector = [0]*num_features
    for token in tokenize(text):
        if token not in features:
            continue
        vector[features[token]] += 1

    return vector

if __name__=="__main__":
    with open(FNAME) as f:
        posts = json.load(f)
        (X, y, features, num_features) = generate_map(posts[:16000])
        clf = svm.SVC()
        clf.fit(X, y)

        correct = 0
        for post in posts[16000:]:
            pred = clf.predict(get_feature_vector(post['text'][1:].decode('hex').decode('utf-8'), features, num_features))[0]
            v = int(post['state'])
            print "---"
            print pred
            print v
            if pred == v:
                correct += 1
                
        print "OPness: "
        print correct
        print float(correct)/(len(posts) - 16000)

