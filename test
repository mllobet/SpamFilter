#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import re
import json
import tldextract

import regex

from nltk.stem import snowball

commonWords = [
    'de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'se', 'del', 'las', 'un',
    'por', 'con', 'no', 'una', 'su', 'para', 'es', 'al', 'lo', 'como', 'mas',
    'o', 'pero', 'sus', 'le', 'ha', 'me', 'si', 'sin', 'sobre', 'este', 'ya',
    'entre', 'cuando', 'todo', 'esta', 'ser', 'son', 'dos', 'tambien', 'fue',
    'habia', 'era', 'muy', 'anos', 'hasta', 'desde', 'esta', 'mi', 'porque',
    'que', 'solo', 'han', 'yo', 'hay', 'vez', 'puede', 'todos', 'asi', 'nos',
    'ni', 'parte', 'tiene', 'el', 'uno', 'donde', 'bien', 'tiempo', 'mismo',
    'ese', 'ahora', 'cada', 'e', 'vida', 'otro', 'despues', 'te', 'otros',
    'aunque', 'esa', 'eso', 'hace', 'otra', 'gobierno', 'tan', 'durante',
    'siempre', 'dia', 'tanto', 'ella', 'tres', 'si', 'dijo', 'sido', 'gran',
    'pais', 'segun', 'de', 'la', 'i', 'el', 'a', 'l', 'en', 'va', 'de', 'que',
    'del', 'un', 'les', 'amb', 'per', 'una', 'els', 'es', 'al', 'es', 'dels',
    'ser', 'van', 'o', 'com', 'mes', 'no', 's', 'seva', 'fou', 'entre', 'pero',
    'seu', 'com', 'per', 'tambe', 'son', 'als', 'on', 'aquest', 'anys',
    'ciutat', 'era', 'pel', 'despres', 'va', 'aquesta', 'part', 'gran', 'ha',
    'durant', 'sobre', 'any', 'nom', 'estat', 'altres', 'havia', 'dos', 'rei',
    'molt', 'primer', 'quan', 'fer', 'li', 'contra', 'pels', 'cap', 'els seus',
    'seves', 'pot', 'temps', 'mateix', 'hi', 'tres', 'forma', 'esta',
    'guerra', 'te', 'despres', 'fins', 'fill',

    'http', 'facebook', 'google', 'com', 'org', 'net', 'cat', 'es', 'www'
];

s = snowball.SpanishStemmer()

def cleanWord(word):
    if word in commonWords:
        return None
    return s.stem(word)


FNAME = "posts.json"


ACCENTS = ((u'à', u'a'), (u'á', u'a'), (u'è', u'e'), (u'é', u'e'), (u'ì', u'i'), 
           (u'í', u'i'), (u'ò', u'o'), (u'ó', u'o'),
           (u'ù', u'u'), (u'ú', u'u'), (u'ñ', 'n'))

def clean(s):
    """ turns string to lowercase and removes all accents """
    s = s.lower()
    for c_old, c_new in ACCENTS:
        s = s.replace(c_old,c_new)
    return s

def tokenize(message):
    urls = re.findall(regex.findUrl, message)
    found_urls = []
    for url in urls:
        found_urls += url.group(0)

    for url in found_urls:
        domain = tldextract.extract(url).domain
        message.replace(url, u"__url" + domain)

    tokens = re.findall(r"[\w']+", message)
    return [clean(t) for t in tokens] 

def run():
    with open(FNAME) as f:
        posts = json.load(f)

    for post in posts:
        data = post['text'][1:].decode('hex')
        data = data.decode('utf-8')
        print tokenize(data)
        #print data.encode('utf-8')

if __name__=="__main__":
    run()
